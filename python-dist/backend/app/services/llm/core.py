def get_llm_response(prompt: str) -> str:
    # Replace this with your real LLM API call (OpenAI, Ollama, etc)
    return f"Wingman says: {prompt[::-1]}"  # Dummy: reverses the prompt